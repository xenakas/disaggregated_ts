{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРОГНОЗИРОВАНИЕ ВРЕМЕННЫХ РЯДОВ С ИЕРАРХИЧЕСКОЙ СТРУКТУРОЙ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical or Grouped Time Series\n",
    "\n",
    "\n",
    "- [x] [Forecasting Economic Aggregates Using Dynamic Component Grouping](https://mpra.ub.uni-muenchen.de/81585/1/MPRA_paper_81585.pdf)\n",
    "\n",
    "- [ ] [Optimal combination forecasts for hierarchical time series](https://robjhyndman.com/papers/Hierarchical6.pdf); [slides](https://robjhyndman.com/talks/Hierarchical%20ISF2006.pdf) \n",
    "\n",
    "- [ ] [Essays in Hierarchical Time Series Forecasting and Forecast Combination](https://core.ac.uk/download/pdf/154428359.pdf)\n",
    "\n",
    "- [ ] [Fast computation of reconciled forecasts for hierarchical and grouped time series](https://robjhyndman.com/papers/hgts4.pdf)\n",
    "\n",
    "- [ ] [Accommodating Small Sample Sizes in Three-Level Models When the Third Level is Incidental](https://www.tandfonline.com/doi/abs/10.1080/00273171.2016.1262236?src=recsys&journalCode=hmbr20)\n",
    "\n",
    "- [ ] [Hierarchical forecasts for Australian domestic tourism](http://webdoc.sub.gwdg.de/ebook/serien/e/monash_univ/wp12-07.pdf)\n",
    "\n",
    "- [ ]  [Grouped time-series forecasting with an application to regional infant mortality counts](http://www.cpc.ac.uk/docs/2013_WP40_Grouped_Time-Series_Forecasting_Shang_et_al.pdf): bottom-up and optimal combination methods, a parametric bootstrap method for constructing point-wise prediction intervals,  one-step-ahead to ten-step-ahead point and interval forecast accuracy.\n",
    "\n",
    "- [ ] [Uncertainty and variability in demography and population growth: a hierarchical approach](http://zero.sci-hub.tw/3978/f6fab7692f0e7bfaf6c1c9662cde415e/clark2003.pdf)\n",
    "\n",
    "- [ ] [Grouped functional time series forecasting: An application to age-specific mortality rates](https://arxiv.org/pdf/1609.04222.pdf)\n",
    "\n",
    "- [ ] [Grouped multivariate and functional time series forecasting: An application to annuity pricing](https://www.sciencedirect.com/science/article/pii/S016766871630484X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] [Time-series clustering – Decade overview](https://www.sciencedirect.com/science/article/abs/pii/S0306437915000733) \n",
    "\n",
    "- [x] [The M3-Competition: results, conclusions and implications](https://pdfs.semanticscholar.org/8461/b79f9747a0caee85522c49bd4655c64e10fb.pdf)\n",
    "\n",
    "\n",
    "\n",
    "- [ ] [Forecasting with group seasonality](https://pure.tue.nl/ws/files/1718873/200610743.pdf)\n",
    "\n",
    "\n",
    "- [ ] [Multilevel (Hierarchical) Modeling: What It Can and Cannot Do](http://www.stat.columbia.edu/~gelman/research/published/multi2.pdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Forecasting analogous time series](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.519.6011&rep=rep1&type=pdf)\n",
    "\n",
    "- [Forecasting output growth rates and median output growth rates: a hierarchical Bayesian approach](https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.800)\n",
    "\n",
    "- [Forecasting Short Time Series with the Bayesian Autoregression and the Soft Computing Prior Information](https://link.springer.com/chapter/10.1007/978-3-319-10765-3_10)\n",
    "\n",
    "- [The use of prior information\n",
    "in forecast combination ](https://www.sas.upenn.edu/~fdiebold/papers/paper94/DieboldPauly1990.pdf) \n",
    "\n",
    "- [x] [Forecasting international growth rates using Bayesian shrinkage and other procedures](https://www.sciencedirect.com/science/article/pii/0304407689900365): ARLI\n",
    "\n",
    "- [Joint Forecast Combination of\n",
    "Macroeconomic Aggregates and Their\n",
    "Components](https://mpra.ub.uni-muenchen.de/76556/1/MPRA_paper_76556.pdf)\n",
    "\n",
    "\n",
    "- [ ] [Forecasting Method for Grouped Time Series with the Use of k-Means Algorithm](https://arxiv.org/pdf/1509.04705.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Взвешенные прогнозы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding optimal forecast:\n",
    "\n",
    "[With three components the feasible set is five: the aggregate, full disaggregation and three options where one component is forecasted on its own and the other two together. With four components the possibilities grow to fifteen and with five components to 52.](https://mpra.ub.uni-muenchen.de/81585/1/MPRA_paper_81585.pdf)\n",
    "\n",
    "- [Many relevant economic aggregates, like GDP and CPI, do not fall in this category and it is unclear whether these methods will work with relatively small samples.](https://mpra.ub.uni-muenchen.de/81585/1/MPRA_paper_81585.pdf)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hts: [pdf](https://cran.r-project.org/web/packages/hts/vignettes/hts.pdf)\n",
    "\n",
    "https://otexts.com/fpp2/hts.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Иерархический Байес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving forecasting accuracy of GDP with Hierarchical Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hb vs eb? Empirical Bayes uses the data to set the hyperparameters of the prior. Performing Bayesian inference with this prior then gets you a sort of shrinkage and can be viewed as an approximation to a hierarchical Bayesian model\n",
    "\n",
    "- hb, empirical bayes (Get prior from aggregated ts: lower level priors based on aggreagted time series or prior for cluster using mean for that cluster), grouped priors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequentist approach:\n",
    "- We made assumptions on the generating process (e.g., i.i.d.,\n",
    "Gaussian data, smooth density, linear regression function,\n",
    "etc...)\n",
    "- The generating process was associated to some object of\n",
    "interest (e.g., a parameter, a density, etc...)\n",
    "- This object was unknown but fixed and we wanted to find it:\n",
    "we either estimated it or tested a hypothesis about this object,\n",
    "etc.\n",
    "\n",
    "The Bayesian approach:\n",
    "\n",
    "- Now, we still observe data, assumed to be randomly generated\n",
    "by some process. Under some assumptions (e.g., parametric\n",
    "distribution), this process is associated with some fixed object.\n",
    "- We have a prior belief about it.\n",
    "- Using the data, we want to update that belief and transform\n",
    "it into a posterior belief. \n",
    "\n",
    "Idea: In case of ignorance, or of lack of prior information, one\n",
    "may want to use a prior that is as little informative as\n",
    "possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическая часть\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавить из описания данных актуальность\n",
    "- точнее считать вклад каждого показателя в агрегат => анализ структуры изменения показтеля\n",
    "\n",
    "\n",
    "http://www.machinelearning.ru/wiki/images/f/fd/2018_June_RZD_presentation.pdf\n",
    "\n",
    "https://publications.hse.ru/mirror/pubs/share/direct/248447556\n",
    "\n",
    "- Цели и задачи, определенные в рамках тем в интересах РЖД \n",
    "Решается проблема повышения эффективности транспортировки грузов. Для решения задачи выполняется прогноз потребностей узаказчиков РЖД в узлах погрузки/разгрузки с учетомвременных интервалов доставки.\n",
    "\n",
    "Требуется построить иерархический прогноз объемов спросав заданном периоде:\n",
    "на месяц посуточно, на месяц подекадно, на квартал помесячно, на год помесячно, на период больше года\n",
    "\n",
    "С расположением: по группам грузов, по парам станций/регионов, по комбинированному разложению, учитывающему перечисленные варианты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Clustering of time series data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.6594&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a solution for classifying enormous data when there is not any early knowledge about classes.\n",
    "\n",
    "Clustering methods can be broadly divided into three main categories: overlapping, partitional,\n",
    "and hierarchical. A hierarchical clustering  method works by grouping data objects into a tree \n",
    "of clusters. There are generally two types of hierarchical clustering methods: \n",
    "agglomerative and divisive. \n",
    "A neural clustering method, the self-organizing map (SOM), is used for pattern discovery. \n",
    "Ghaseminezhad  presented a novel SOM-based algorithm that can automatically cluster \n",
    "discrete groups of data using an unsupervised method. \n",
    "Hidden Markov model (HMM) is a common model based algorithm adopted in time series clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Clustering of Time Series Subsequences is Meaningless](http://www.cs.ucr.edu/~eamonn/meaningless.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whole Clustering: The notion of clustering here is similar to that of conventional clustering of discrete objects. Given a set of individual time series data, the objective is to group similar time series into the same cluster.\n",
    "- Subsequence Clustering: Given a single time series, sometimes in the form of streaming time series, individual time series (subsequences) are extracted with a sliding window. Clustering is then performed on the extracted time series subsequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work we make a surprising claim. Clustering of time series subsequences is meaningless! In particular,\n",
    "clusters extracted from these time series are forced to obey a certain constraints that are pathologically unlikely to be\n",
    "satisfied by any dataset, and because of this, the clusters extracted by any clustering algorithm are essentially\n",
    "random.\n",
    "\n",
    "Since we use the word “meaningless” many times in this paper, we will take the time to define this term. All\n",
    "useful algorithms (with the sole exception of random number generators) produce output that depends on the input.\n",
    "For example, a decision tree learner will yield very different outputs on, say, a credit worthiness domain, a drug\n",
    "classification domain, and a music domain. We call an algorithm “meaningless” if the output is independent of the\n",
    "input. As we prove in this paper, the output of STS clustering does not depend on input, and is therefore\n",
    "meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background on Clustering\n",
    "\n",
    "Algorithm Hierarchical Clustering\n",
    "1. Calculate the distance between all objects. Store the results in a distance matrix.\n",
    "2. Search through the distance matrix and find the two most similar clusters/objects.\n",
    "3. Join the two clusters/objects to produce a cluster that now has at least 2 objects.\n",
    "4. Update the matrix by calculating the distances between this new cluster and all other clusters.\n",
    "5. Repeat step 2 until all cases are in one cluster.\n",
    "\n",
    "Algorithm k-means\n",
    "1. Decide on a value for k.\n",
    "2. Initialize the k cluster centers (randomly, if necessary).\n",
    "3. Decide the class memberships of the N objects by assigning them to the nearest cluster center.\n",
    "4. Re-estimate the k cluster centers, by assuming the memberships found above are correct.\n",
    "5. If none of the N objects changed membership in the last iteration, exit. Otherwise go to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these two numbers to create a fraction:\n",
    "between set X and Y distance\n",
    "within set X distance\n",
    "\n",
    "$$ clustering \\ meaningfulness = \\frac{within \\ set \\ X \\ distance}{between \\ set \\ X \\ and \\ Y  \\ distance} $$\n",
    "\n",
    "We can justify calling this number “clustering meaningfulness” since it clearly measures just that. If, for any dataset,\n",
    "the clustering algorithm finds similar clusters each time regardless of the different initial seeds, the numerator should\n",
    "be close to zero. In contrast, there is no reason why the clusters from two completely different, unrelated datasets\n",
    "should be similar. Therefore, we should expect the denominator to be relatively large. So overall we should expect\n",
    "that the value of clustering meaningfulness  be close to zero when X and Y are sets of cluster centers derived\n",
    "from different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implications of Theorem 1 become clearer when we consider the following well documented fact. For any\n",
    "dataset, the weighted (by cluster membership) average of k clusters must sum up to the global mean. The implication\n",
    "for STS clustering is profound. Since the global mean for STS clustering is a straight line, then the weighted average\n",
    "of k-clusters must in turn sum to a straight line. However, there is no reason why we should expect this to be true of\n",
    "any dataset, much less every dataset. This hidden constraint limits the utility of STS clustering to a vanishing small\n",
    "set of subspace of all datasets. The out-of-phase sine waves as cluster centers that we get from the last section\n",
    "conforms to this theorem, since their weighted average, as expected, sums to a straight line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even more tantalizing piece of evidence exists. In the 1920’s “data miners” were excited to find that by\n",
    "preprocessing their data with repeated smoothing, they could discover trading cycles. Their joy was shattered by a\n",
    "theorem by Evgeny Slutsky (1880-1948), who demonstrated that any noisy time series will converge to a sine wave\n",
    "after repeated applications of moving window smoothing (Kendall, 1976). While STS clustering is not exactly the\n",
    "same as repeated moving window smoothing, it is clearly highly related. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm motif-based-clustering\n",
    "1. Decide on a value for k.\n",
    "2. Discover the K-motifs in the data, for K = k × c (c is some constant, in the region of about 2 to 30)\n",
    "3. Run k-means, or k partitional hierarchical clustering, or any other clustering algorithm on the subsequences covered by Kmotifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Hierarchical Clustering](https://www.datacamp.com/community/tutorials/hierarchical-clustering-R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking there are two ways of clustering data points based on the algorithmic structure and operation, namely agglomerative and divisive.\n",
    "\n",
    "- Agglomerative : An agglomerative approach begins with each observation in a distinct (singleton) cluster, and successively merges clusters together until a stopping criterion is satisfied.\n",
    "- Divisive : A divisive method begins with all patterns in a single cluster and performs splitting until a stopping criterion is met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing operations for Clustering:\n",
    "\n",
    "Scaling\n",
    "\n",
    "It is imperative that you normalize your scale of feature values in order to begin with the clustering process. This is because each observations' feature values are represented as coordinates in n-dimensional space (n is the number of features) and then the distances between these coordinates are calculated. If these coordinates are not normalized, then it may lead to false results.\n",
    "\n",
    "Other type of scaling can be achieved via the following transformation:\n",
    "\n",
    "$$x(s)=x(i)−mean(x)/sd(x)$$\n",
    "\n",
    "Where $sd(x)$ is the standard deviation of the feature values. This will ensure your distribution of feature values has mean 0 and a standard deviation of 1. You can achieve this via the scale() function in R.\n",
    "\n",
    "\n",
    "Missing Value imputation\n",
    "\n",
    "It's also important to deal with missing/null/inf values in your dataset beforehand. There are many ways to deal with such values, one is to either remove them or impute them with mean, median, mode or use some advanced regression techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key operation in hierarchical agglomerative clustering is to repeatedly combine the two nearest clusters into a larger cluster. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to measure the distance between clusters in order to decide the rules for clustering, and they are often called Linkage Methods. Some of the common linkage methods are:\n",
    "- Complete-linkage: calculates the maximum distance between clusters before merging.\n",
    "- Single-linkage: calculates the minimum distance between the clusters before merging. This linkage may be used to detect high values in your dataset which may be outliers as they will be merged at the end.\n",
    "- Average-linkage: calculates the average distance between clusters before merging.\n",
    "- Centroid-linkage: finds centroid of cluster 1 and centroid of cluster 2, and then calculates the distance between the two before merging.\n",
    "\n",
    "The choice of linkage method entirely depends on you and there is no hard and fast method that will always give you good results. Different linkage methods lead to different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering\n",
    "- Calculate the distance between all objects. Store the-\n",
    "results in a distance matrix.\n",
    "- Search through the distance matrix and find the two\n",
    "most similar clusters/objects.\n",
    "- Join the two clusters/objects to produce a cluster that\n",
    "now has at least 2 objects.\n",
    "- Update the matrix by calculating the distances\n",
    "between this new cluster and all other clusters.\n",
    "- Repeat step 2 until all cases are in one cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Algorithms for hierarchical clustering](https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative hierarchical clustering has been the\n",
    "dominant approach in constructing embedded\n",
    "classification schemes. It is our aim to direct the read-\n",
    "er’s attention to practical algorithms and methods—\n",
    "both efficient (from the computational and storage\n",
    "points of view) and effective (from the application\n",
    "point of view). It is often helpful to distinguish\n",
    "between method, involving a compactness criterion\n",
    "and the target structure of a two-way tree represent-\n",
    "ing the partial order on subsets of the power set, as\n",
    "opposed to an implementation, which relates to the\n",
    "details of the algorithm used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STORED DATA APPROACH\n",
    "- Examine all interpoint dissimilarities, and form cluster from two closest points.\n",
    "- Replace two points clustered by representative point (center of gravity) or by cluster fragment.\n",
    "- Return to step 1, treating clusters as well as remaining objects, until all objects are in one cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEAREST NEIGHBOR CHAIN ALGORITHM\n",
    "- Select a point arbitrarily.\n",
    "- Grow the NN chain from this point until a pair of RNNs is obtained.\n",
    "- Agglomerate these points (replacing with a cluster point, or updating the dissimilarity matrix).\n",
    "- From the point which preceded the RNNs (or from any other arbitrary point if the first two points chosen in steps 1 and 2 constituted a pair of RNNs), return to step 2 until only one point remains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONTIGUITY CONSTRAINED HIERARCHICAL CLUSTERING\n",
    "- Consider each observation in the sequence as constituting a singleton cluster. Determine the closest pair of adjacent observations, and define a cluster from them.\n",
    "- Determine and merge the closest pair of adjacent clusters, $c_1$ and $c_2$, where closeness is defined by $d(c_1 , c_2 ) = \\max {d_{ii} \\  s. t. \\ i \\in c_1, i \\in c_2 }$.\n",
    "- Repeat step 2 until only one cluster remains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other methods:\n",
    "- Hierarchical self-organizing maps and hierarchical mixture modeling \n",
    "- Grid- and density-based  clustering techniques \n",
    "- Linear time grid clustering method: m-ADIC clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of cluster quality\n",
    "\n",
    "In the external approach, groups\n",
    "assembled by a clustering algorithm are compared to\n",
    "a previously accepted partition on the testing dataset.\n",
    "In the internal approach, clustering validity is evalu-\n",
    "ated using data and features contained in the dataset.\n",
    "In all cases,\n",
    "validity indices are constructed to evaluate proximity\n",
    "among objects in a cluster or proximity among\n",
    "resulting clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Distance measures](https://arxiv.org/pdf/1710.02268.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among  the  dissimilarity  methods  tested,  those  which  provide  the  most  robust  results  to  classify  time  series  are:  Euclidean distance, Correlation (COR), Raw Values and Temporal Correlation (CORT) and Dynamic Time Warping (DTW).In addition, a complexity-based approach  called Complexity Invariant Distance (CID) is applied. With this method,instead  of  focusing  on  the  shape  of  the  series,  we  expect  to group profiles from a different perspective, taking into account the degree of variability over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic  Time  Warping  (DTW): DTW  is  a non-linear similarity  measure  obtained  by  minimizing  the  distance  between  two  time  series.  This  method  permits  to  group together series that have similar shape but out of phase. DTW distance can be expressed as\n",
    "$$DTW(X,Y) = \\min_{r\\in M}(\\sum^M_{m=1}|x_{im}−y_{jm}|)$$\n",
    "where  the  path  element $r= (i,j)$ represents  the  relationship between  the  two  series.  The  goal  is  to  minimize  the  time warping path $r$ so that summing its $M$ components gives the lowest measure of minimum cumulative distance between the time series. DTW searches for the best alignment between $X$ and $Y$,  computing  the  minimum  distance  between  the  points $x_i$ and $y_j$.\n",
    "\n",
    "DTW  works  particularly  well  to  group  similar  player profiles with a shift on the time axis. DTW also groups together similar patterns but at different scale. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation-based measure (COR): It performs dissimilarities  based  on  the  estimated  Pearson’s  correlation  of  two given time series. The COR computation can be expressed as\n",
    "$$COR(X,Y) =\\frac{\\sum^N_{n=1}(x_n−\\bar{X})(y_n−\\bar{Y})}{\\sqrt{\\sum^N_{n=1}(x_n−\\bar{X})^2}\\sqrt{\\sum^N_{n=1}(y_n−\\bar{Y})^2}}$$\n",
    "\n",
    "COR seems  to  be  sensitive  to  noise  data  and  outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal  Correlation  and  Raw  Values  Behaviors  measure  (CORT): It  computes  an  adaptive  index  between  two time  series  that  covers  both  dissimilarity  on  raw  values  and dissimilarity  on  temporal  correlation  behaviors.  \n",
    "$$CORT(X,Y) =\\frac{\\sum^{N-1}_{n=1}(x_{n+1}-x_n)(y_{n+1}-y_n)}{\\sqrt{\\sum^{N-1}_{n=1}(x_{n+1}-x_n)^2}\\sqrt{\\sum^{N-1}_{n=1}(y_{n+1}-y_n)^2}}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model-based clustering, the data are viewed as coming from a distribution that is mixture of two ore more clusters. It finds best fit of models to data and estimates the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Classification Of Short Time Series](https://www.researchgate.net/publication/46447515_Classification_Of_Short_Time_Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Abstract:__ In this paper, we consider several ways of assigning a dissimilarity between univariate time series in short term\n",
    "behavior. In particular, we have defined a measure that works irrespective of different baselines and scaling factors and its effectiveness has been evaluated on real\n",
    "and synthetic data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic problems in handling short time series involve the clustering of such series into similar categories and the classification of new observed series into two or more known categories. \n",
    "\n",
    "These two problems, of course, are very common and there exists a\n",
    "vast literature on methods of discriminant and cluster analysis as applied to time\n",
    "independent observations. \n",
    "\n",
    "The basic idea is to extract distinctive features from\n",
    "the data, compare them and perform the grouping of the units into distinct categories. The clustering is satisfactory if the distance between units within clusters\n",
    "is relatively small compared with distances between clusters. Once the structure\n",
    "and the required number of clusters have been established, the cluster representatives can be employed to classify the old and new units using, for example, the\n",
    "nearest-centroid method.\n",
    "\n",
    "Clustering methods can identify meaningful patterns even in time dependent\n",
    "observations; however, they have some limitations if standard algorithms are blindly applied measuring the closeness of the observed values, but ignoring the temporal dimension. In this case, one wants to assign a value to the distance between individual time series rather than quantify the strength of relationship between the stochastic processes that generate the observations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A new metric for short time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present our approach, we consider time series in which the values have been\n",
    "standardized to have a mean of zero and a standard deviation of one. This type of\n",
    "invariance is useful for dealing with heterogeneity of scales and/or baseline shift\n",
    "when one considers meaningless to compare sequences with different levels and\n",
    "oscillations. \n",
    "\n",
    "Our method\n",
    "is based on the observation that similar sequences will have a small area enclose between the polygonal curves representing them\n",
    "\n",
    "However, for a distance function to be useful in the\n",
    "context of time series, it should be driven by the relative change of intensity in a\n",
    "given interval as well as the time order of observations.\n",
    "\n",
    "$$WDA(X_i,X_j)=\\frac{\\sum w_tA_t}{\\sum w_t}$$\n",
    "\n",
    "$w_t = 0.5(\\min\\{ |x_{it}-x_{jt}|, |x_{it+1} - x_{jt+1}| \\} + \\max\\{ |x_{it}-x_{jt}|, |x_{it+1} - x_{jt+1}| \\})$\n",
    "\n",
    "However, its results are very similar to those of the Euclidean\n",
    "distance; in practice, WAD does not bring massive improvement over the\n",
    "current State of the Art. At the very least, one should wonder if it just makes\n",
    "sense to introduce a product if it is only slightly better than the current best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative metrics for short time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest distance function can be expressed by the __Minkowsky formula__:\n",
    "\n",
    "$$M_a(X_i,X_j)=\\left( \\sum (x_{it} -x_{jt})^a  \\right)^{1/a}$$\n",
    "\n",
    "\n",
    "- Manhattan for $a = 1$\n",
    "- Euclidean for $a = 2$\n",
    "- Tchebycheff for $a \\to \\infty$, putting different emphasis on large deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the Minkowky metrics ignore the temporal structure of the values as resemblance is only based on the differences between the values, independently of the\n",
    "increase/decrease behavior before and after these values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noted that if the variables are standardized then the Euclidean metric, a = 2, is functionally related to the correlation metric \n",
    "\n",
    "$$M_2 (X_i, X_j) = \\sqrt{2n(1-r(X_i, X_j))}$$\n",
    "\n",
    "where $r(X_i, X_j)$ is the __Pearson’s cross-product correlation coefficient__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The strength of monotonic association between two set of values can be measured by a rank-based measure of correlation: __the Kendall’s concordance coefficient__\n",
    " \n",
    "$$\\tau (X_i, X_j) = 1 - \\frac{2d_k(X_i, X_j)}{n(n-1)}$$\n",
    "\n",
    "where $d_k(X_i, X_j)$ can be interpreted as the symmetric difference distance between the ordered values of $X_i$ and $X_j$ and used as a metric to classify time series.  The\n",
    "Kendall’s $\\tau$ gives the probability that any two corresponding pairs of values in the\n",
    "two vectors are identically ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resemblance between polygonals can also be measured by a discrete version of the __[Frechet distance](https://en.wikipedia.org/wiki/Fr%C3%A9chet_distance)__. The discrete Frechet distance is similar to the __dynamic time warping distance__ (DTW). It must be observed that, in our context,\n",
    "the Frechet distance coincides with $M_{\\infty}(X_i,X_j)$, so that we could expect results similar to the Euclidean metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Cluster analysis\n",
    "\n",
    "We chose the partitioning around medoids method (PAM)\n",
    "for several reasons. \n",
    "- First, the typical representative of each group (the cluster medoid) is the most centrally located item in a cluster, that is, the item in the cluster whose average dissimilarity to all other items in the same cluster is minimal. \n",
    "- Second, it can operate directly on a distance matrix. In fact, the computation of cluster medoids does not require the presence of feature vectors, but can be done for a distance matrix. \n",
    "- Third, it is a partitional algorithm that does not impose a hierarchical structure, which is not necessarily present in the underlying hypothetical population. \n",
    "- Fourth, rather than selecting starting centers at random, PAM evaluates all possible starting centers and chooses the best centers to start cluster building. This gives consistent results when clustering is repeated. \n",
    "- Finally, PAM has been shown to be both more robust to inclusion of outliers than the popular k-means method because it uses the most centrally located object in a cluster\n",
    "\n",
    "\n",
    "The result of PAM is a partition of the original set in k categories each formed by\n",
    "time sequences with similar dynamics, and each guided by a leading sequence that\n",
    "explains how the group is shaped, how it evolves over time and what sequences\n",
    "we are to expect to be included in the group. In addition, PAM returns for each\n",
    "unit a silhouette width that reflects how well the particular unit is clustere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
